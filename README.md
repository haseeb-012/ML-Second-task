  
# Task 01: Classification Fundamentals and MNIST Digit Recognition

# Task 02: Regression Fundamentals and California Housing Price Prediction

## ğŸ“ˆ Task 02 Overview

This section covers **Task 02: Regression Fundamentals and California Housing Price Prediction** for the ML Internship at ARCH Technologies. The project applies Chapter 2 regression concepts to predict housing prices using the California Housing dataset.

## ğŸ“‹ Task 02 Requirements

âœ… **All requirements successfully completed:**
- [x] Chapter 2 study and exercises
- [x] California Housing dataset loading and preprocessing
- [x] Linear Regression and Decision Tree Regression model training
- [x] Achieve RMSE â‰¤ 0.7 on test set
- [x] Performance comparison tables (Linear vs Tree)
- [x] Error analysis and feature importance
- [x] Model improvement (feature engineering or ensemble)
- [x] Gradio web application deployment

## ğŸ† Task 02 Results Summary

| Metric         | Linear Regression | Decision Tree | Target   |
|----------------|------------------|--------------|----------|
| **Test RMSE**  | 0.74             | **0.65** âœ…   | â‰¤0.7     |
| **Training Time** | ~2s           | ~10s         | -        |
| **Status**     | Slightly above   | **Target Achieved** | âœ… |

## ğŸš€ Task 02 Quick Start

1. Install dependencies (already included in requirements.txt)
2. Run `Chapter2-Regression.ipynb` for full workflow
3. Launch Gradio app: `python app_regression.py`

## ğŸ“ Task 02 Repository Structure

```
â”œâ”€â”€ Chapter2-Regression.ipynb         # Regression implementation notebook
â”œâ”€â”€ Task02_Regression_Report.md       # Project report for Task 02
â”œâ”€â”€ app_regression.py                 # Gradio web application for regression
```

## Key Results:
- End-to-end California Housing price prediction pipeline
- Multiple regression model comparisons and evaluations
- Error analysis and feature importance insights
- Deployable web application for price prediction
- All Task 02 requirements satisfied


## ğŸ¯ Project Overview

This repository contains the complete implementation of **Task 01: Classification Fundamentals and MNIST Digit Recognition** for the ML Internship at ARCH Technologies. The project demonstrates mastery of Chapter 3 classification concepts through practical MNIST digit recognition implementation.

## ğŸ“‹ Task Requirements

âœ… **All requirements successfully completed:**
- [x] Chapter 3 study and exercises
- [x] MNIST dataset implementation (60k train, 10k test)
- [x] SGD and Random Forest classifier training
- [x] Achieve â‰¥95% test accuracy
- [x] Performance comparison tables (SGD vs RF, OvR vs OvO)
- [x] Error analysis with pattern identification
- [x] Model improvement implementation
- [x] Gradio web application deployment

## ğŸ† Results Summary

| Metric | SGD Classifier | Random Forest | Target |
|--------|----------------|---------------|---------|
| **Test Accuracy** | 91.2% | **97.1%** âœ… | â‰¥95% |
| **Training Time** | ~5s | ~45s | - |
| **Status** | Below target | **Target Achieved** | âœ… |

## ğŸ“ Repository Structure

```
â”œâ”€â”€ Chapter3-Classification.ipynb     # Main implementation notebook
â”œâ”€â”€ Task01_Classification_Report.md   # Comprehensive project report  
â”œâ”€â”€ app.py                           # Gradio web application
â”œâ”€â”€ task01_demo.py                   # Standalone demonstration script
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Jupyter Notebook
```bash
jupyter notebook Chapter3-Classification.ipynb
```

### 3. Launch Web Application
```bash
python app.py
```
- Error Analysis with systematic error identification

#### 2. MNIST Digit Recognition Project âœ…
- **(a)** Load MNIST dataset âœ…
- **(b)** Split data (60k train, 10k test) âœ…
- **(c)** Train classifiers:
  - SGD Classifier (with hinge loss) âœ…
  - Random Forest Classifier âœ…
- **(d)** Evaluate using confusion matrix & classification report âœ…
- **(e)** Visualize errors (plot worst misclassifications) âœ…
- **(f)** Deploy as Gradio web app âœ…
- **Target**: Achieve minimum 95% test accuracy âœ…

#### 3. Error Analysis Report âœ…
- Identify 3 common error patterns âœ…
- Propose solutions (data augmentation, preprocessing) âœ…
- Implement one improvement (ensemble method) and measure impact âœ…

#### 4. Comparison Tables âœ…
- SGD Classifier vs Random Forest performance âœ…
- OvR vs OvO strategies for multiclass âœ…

### Expected Deliverables:

#### PDF Report âœ…
- Chapter 3 exercise solutions + MNIST Digit Recognition Project
- Error analysis findings
- Training/validation curves
- Performance comparison tables

#### GitHub Repository âœ…
- Jupyter notebooks (data exploration + training)
- Gradio app code (app.py + requirements.txt)
- Standalone demo script

### How to Run:

1. **Notebook**: Open `Chapter3-Classification.ipynb` and run cells sequentially
2. **Demo Script**: `python task01_demo.py`
3. **Web App**: `python app.py` (after running notebook to generate model files)

### Key Results:
- Comprehensive MNIST digit classification implementation
- Multiple classifier comparisons and evaluations
- Error analysis with improvement strategies
- Deployable web application for digit recognition
- All Task 01 requirements satisfied
